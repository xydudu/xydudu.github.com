---

layout: default
title: Socket.io 最佳实践
date: 2012-08-13 

---

最近我在公司内部上线了一个服务，用于实时推送一些消息给用户，项目名为 Honey.Pusher，源码开源，且托管在 [Github](https://github.com/xydudu/Honey-pusher)。如果你有兴趣，你可以clone一份代码，有改动的话，尽管的pull request。

折腾这个项目差不多有两周整，主要依赖于 [socket.io](https://github.com/LearnBoost/socket.io) 来实现的，大部分的工作都是在服务器端的，因为 socket.io 在高并发的情况下，稳定性与可靠性其实是不高的。特别是其websocket的实现，websocket本身就是存在很多局限的，对服务端，客户端的要求都比较高，要使其实稳定的运行，还是有蛮多的工作要做的。

另外，socket.io 的作者又开发了另一个类似的实现：[engine.io](https://github.com/LearnBoost/engine.io) 应该是就socket.io的一些不足作了改进，但是我还没来得及研究，不过可以先阅读他就此写的文章：[The Realtime Engine](http://www.devthought.com/2012/07/07/the-realtime-engine/) 大概就能明白一些缘由。


此文着重要讲的当然是如何在高并发的生产环境中架构一个socket.io服务。这其实基本上就是服务器端的事了。

## Proxy Socket.io
你可能直接把nodejs listen到80端口，网站，socket.io服务都运行在此进程中。然后就可以让用户直接访问了，嗯，这其实是没有问题的。这里暂且不表。

但是更多的，你的服务器80商品是被占用掉了，于是你得想办法把socket.io的服务proxy到80端口去，以便让用户不用加难看的端口号来访问。而此时，你就会遇到一个很大的坑，因为你可能第一时间就会想到用 Nginx 来 proxy_pass。这真是一件不幸的事情，因为现在发布的Nginx是不支持websocket的。这个我不想去解释太多，因为有很多的文章都很详细的解释了，并有一些解决办法。虽然，我并不赞同你真的去使用Nginx来运行你的Socket.io。

解决Nginx Proxy Socket.io的参考：
	
* [Nginx and Socket.io](https://github.com/LearnBoost/socket.io/wiki/Nginx-and-Socket.io)
* [Reverse proxy websocket](http://www.letseehere.com/reverse-proxy-web-sockets)

文章中提到的解决办法简单来说就是使用一个nginx模块：[tcp_module](https://github.com/yaoweibin/nginx_tcp_proxy_module.git)。很可喜的，这是个中国人写的模块，就编译完后的nginx配置，我还写邮件请教过他，他也很热情的回信，很是感谢他。他回信中提到的配置在此：[nginx_tcp_module_config](https://github.com/yaoweibin/nginx_tcp_proxy_module/wiki/websocket)。

用此模块配置的关键就是你的http server端口与websocket端口是不能想同的。总之，这个方法的可操作性不高，首先，编译，配置nginx就很麻烦，而则局限性也大，基本上你的nginx上就只能跑socket.io服务了，这样的话，还不如直接socket.io listen到80来的方便与可靠。所以这种方式，我弃用了。

放弃了Nginx，我尝试着用Nodejs来自己来proxy，于是我找到了 [nodejitsu / node-http-proxy](https://github.com/nodejitsu/node-http-proxy)。可以看我的实现代码：[https://github.com/xydudu/Honey-pusher/blob/master/src/run.coffee](https://github.com/xydudu/Honey-pusher/blob/master/src/run.coffee)。

{%highlight javascript linenos%}
	httpProxy = require 'http-proxy'
	http = require 'http'
	s = new httpProxy.HttpProxy {
    target:
        host: 'localhost'
        port: 9999
	}

	proxyServer = http.createServer (req, res)->
		if req.url.match /socket.io/
        s.proxyRequest req, res
    else
        res.writeHead(200, {'Content-Type': 'text/plain'})
        res.end('okay')
	proxyServer.on 'upgrade', (req, socket, head)->
    s.proxyWebSocketRequest(req, socket, head)

	proxyServer.listen(80)
{%endhighlight%}

这段代码就是http-proxy对外暴露80端口，然后根据url的不同分发到不同的worker来处理请求。于是把www服务与socket.io服务分开了，它们分别使用不同的端口。而用户的请求是经过run.js来进行分发的。

这样的处理是没问题的，能成功的运行，但是当并发上去后，就会发现socket.io的进程CPU与内存使用率非常的高，然后就崩溃了。这当然有服务器的问题，我使用的服务器是8核2G内存，你大可以用更好的服务器来支撑起这个服务。但是这样显然是太不负责了，因为这明显在代码上是可以做优化的。

## Cluster

前面一直是便socket.io以单线程的方式在提供着服务，此进程只占着一个CPU，当并发高时，这个CPU必然是很耗体力的。但是我们的服务器有8个核啊。所以了解nodejs的process是很有用处的，由此，你可以使用nodejs自带的cluster或者其它一些开源的库来处理这个事，比如我使用的是：[mixture](https://github.com/dshaw/mixture)。然后看我的代码: [run.coffee](https://github.com/xydudu/Honey-pusher/blob/master/new/run.coffee)

mix = require('mixture').mix('epic')
bouncy = require 'bouncy'
configs = require './config'

api_port = configs.apiport
socket_port = configs.socketport
node_id = 0
ports = []

sio = mix.task 'socket.io', filename: configs.sio_runner
api = mix.task 'api', filename: configs.api_runner

for i in [0..configs.numCPUs]
    socket_port = socket_port + 1
    node_id = node_id + 1
    ports.push socket_port

    sio.fork args: [socket_port, node_id]

api.fork args: [api_port]

b = bouncy (req, bounce)->
    if req.url.match /^\/socket.io/
        bounce ports[Math.random()*ports.length|0]
    else
        bounce api_port

b.listen configs.port












